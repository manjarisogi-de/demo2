Generate a complete PySpark ETL script from scratch based on the schema below.

Requirements:
- Use clean, production-quality PySpark
- Use col() for all column references
- Use alias() to flatten nested fields
- Normalize timestamps using to_timestamp()
- Explode ONLY these array-of-struct fields (if present): None
- Cast numeric fields to correct types
- Apply simple non-null filters
- Write Parquet with overwrite mode and snappy compression
- No placeholder column names
- No invented fields

Schema:
- request_id: string
- user.id: string
- user.segment: string
- event.type: string
- event.ts: string
- amount: double

Output only the PySpark code. Do not repeat the instructions.
Begin your answer with 'from pyspark.sql'.