
You are an expert Data Engineer. Improve and productionize the PySpark ETL code below.

The ETL must:
- handle nested fields correctly
- normalize timestamps
- handle arrays using explode (if applicable)
- apply light data quality checks
- include clear comments
- write clean, maintainable, production-ready code

=== Inferred Schema ===
{
  "request_id": "string",
  "user.id": "string",
  "user.segment": "string",
  "event.type": "string",
  "event.ts": "string",
  "amount": "double"
}

=== Draft ETL Code ===
None

=== Sample Flattened Row ===
{
  "request_id": "abc123",
  "user.id": "u1",
  "user.segment": "test",
  "event.type": "click",
  "event.ts": "2025-01-25T10:00:00Z",
  "amount": 19.99
}

=== Suggested Data Quality Checks ===


Return only the final improved PySpark ETL script.
